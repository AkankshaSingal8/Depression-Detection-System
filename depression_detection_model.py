# -*- coding: utf-8 -*-
"""Copy of dprsn_dtctn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jHMg-Rc4yrS9SvYx1Y2_Uf-IpR7jeXlN
"""

import os
import zipfile
import json
import re
import nltk
import pandas as pd
from bs4 import BeautifulSoup
import langdetect
from langdetect import detect
from sklearn.metrics import accuracy_score
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.corpus import words
from nltk.util import ngrams
from sklearn import svm
from sklearn.feature_extraction.text import TfidfVectorizer

nltk.download('punkt')

nltk.download('stopwords')

# Download the English words corpus
nltk.download('words')

#Extracting the data
with zipfile.ZipFile('dataset_tweets_rm.zip', 'r') as zip_ref:
    zip_ref.extractall()

# Create empty lists to hold the positive and negative tweet texts
positive_tweets = []
negative_tweets = []

# Loop over the files in the positive folder and extract the tweet text
english_words = set(words.words())
for filename in os.listdir('dataset_tweets_rm/positive/tweet'):
    with open(os.path.join('dataset_tweets_rm/positive/tweet', filename), 'r') as f:
        tweet = json.load(f)
        text = tweet['text']
        # Clean the text by removing hyperlinks, escape sequences, etc.
        text = re.sub(r'http\S+', '', text)
        text = BeautifulSoup(text, 'html.parser').get_text()
        # Remove non-English characters and symbols
        text = re.sub(r'[^\x00-\x7F]+', ' ', text)
        text = re.sub(r'[^\w\s]', ' ', text)
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        text = text.replace('\n', ' ')
        text = text.replace('\\', '')
        # Tokenize the text and remove stop words
        tokens = word_tokenize(text.lower())
        tokens = [word for word in tokens if not word in english_words]
        # Append the cleaned and tokenized text to the positive_tweets list
        positive_tweets.append(' '.join(tokens))

# english_words = set(words.words())
# positive = []
# # Clean the text
# for text in negative_tweets:
#     words_list = text.lower().split()  # Split the text into words and convert to lowercase
#     token = [word for word in words_list if word in english_words]
#     positive += token # Keep only English words
#     bigrams = ngrams(token, 2) #Add 2 word and 3 word phrases 
#     trigrams = ngrams(token, 3) 
#     positive += [ ' '.join(grams) for grams in bigrams ] + [ ' '.join(grams) for grams in trigrams ] 
# # positive

type(positive_tweets)

# Loop over the files in the negative folder and extract the tweet text
english_words = set(words.words())
for filename in os.listdir('dataset_tweets_rm/negative/tweet'):
    with open(os.path.join('dataset_tweets_rm/negative/tweet', filename), 'r') as f:
        tweet = json.load(f)
        text = tweet['text']
        # Clean the text by removing hyperlinks, escape sequences, etc.
        text = re.sub(r'http\S+', '', text)
        text = BeautifulSoup(text, 'html.parser').get_text()
        text = text.replace('\n', ' ')
        text = text.replace('\\', '')
        # Remove non-English characters and symbols
        text = re.sub(r'[^\x00-\x7F]+', ' ', text)
        text = re.sub(r'[^\w\s]', ' ', text)
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        try:
            language = detect(text)
            if language == 'en':
                # Tokenize the text and remove stop words
                tokens = word_tokenize(text.lower())
                tokens = [word for word in tokens if not word in english_words]
                # Append the cleaned and tokenized text to the negative_tweets list
                negative_tweets.append(' '.join(tokens))
        except:
            pass

# english_words = set(words.words())
# negative = []
# # Clean the text
# for text in negative_tweets:
#     words_list = text.lower().split()  # Split the text into words and convert to lowercase
#     token = [word for word in words_list if word in english_words]
#     negative += token # Keep only English words
#     bigrams = ngrams(token, 2) #Add 2 word and 3 word phrases 
#     trigrams = ngrams(token, 3) 
#     negative += [ ' '.join(grams) for grams in bigrams ] + [ ' '.join(grams) for grams in trigrams ] 
# # negative

# Create a list of labels for the positive and negative tweet texts
labels = [1] * len(positive_tweets) + [0] * len(negative_tweets)

# Concatenate the positive and negative tweet texts into a single list
all_tweets = positive_tweets + negative_tweets

# Calculate TF-IDF scores for the tweet texts
tfidf_vectorizer = TfidfVectorizer(ngram_range=(2,4))
tfidf_matrix = tfidf_vectorizer.fit_transform(all_tweets)
# Get the words corresponding to the columns in the matrix
feature_names = tfidf_vectorizer.get_feature_names_out()

# Iterate through each row in the matrix and print the word with the highest TF-IDF value
for i in range(len(all_tweets)):
    row = tfidf_matrix[i].toarray()[0]
    max_tfidf_idx = row.argmax()
    max_tfidf = row[max_tfidf_idx]
    word = feature_names[max_tfidf_idx]
    print(f"Highest TF-IDF word for tweet {i}: {word} ({max_tfidf:.3f})")

tfidf_matrix

# Get the vocabulary and label encoding from the vectorizer
vocab = tfidf_vectorizer.vocabulary_
label_encoding = tfidf_vectorizer.vocabulary_ 

# Convert the sparse matrix to a dense matrix and create a dataframe
dense_matrix = pd.DataFrame(tfidf_matrix.todense(), columns=sorted(vocab.keys()))
dense_matrix['label'] = [1]*len(positive_tweets) + [0]*len(negative_tweets)
dense_matrix = dense_matrix[['label'] + sorted(vocab.keys())]

# Print the dataframe
print(dense_matrix.head())

from sklearn.model_selection import train_test_split

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, labels, test_size=0.2, random_state=42)

# X_train and X_test contain the TF-IDF vectors for the training and testing data
# y_train and y_test contain the corresponding labels (0 for negative, 1 for positive)

from sklearn.naive_bayes import MultinomialNB
# Train a Naive Bayes classifier
nb_clf = MultinomialNB()
nb_clf.fit(X_train, y_train)
nb_preds = nb_clf.predict(X_test)
nb_acc = accuracy_score(y_test, nb_preds)
print("Naive Bayes accuracy:", nb_acc)

from sklearn import svm

# Train a SVM classifier
svm_clf = svm.SVC(kernel='linear')
svm_clf.fit(X_train, y_train)
svm_preds = svm_clf.predict(X_test)
svm_acc = accuracy_score(y_test, svm_preds)
print("SVM accuracy:", svm_acc)

from sklearn.linear_model import LogisticRegression

# Train a logistic regression classifier
lr_clf = LogisticRegression()
lr_clf.fit(X_train, y_train)
lr_preds = lr_clf.predict(X_test)
lr_acc = accuracy_score(y_test, lr_preds)
print("Logistic Regression accuracy:", lr_acc)

from sklearn.neighbors import KNeighborsClassifier

# Train a kNN classifier
knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_train)
knn_preds = knn_clf.predict(X_test)
knn_acc = accuracy_score(y_test, knn_preds)
print("kNN accuracy:", knn_acc)

# Select the best performing classifier based on accuracy
best_acc = max(nb_acc, svm_acc, lr_acc, knn_acc)
if best_acc == nb_acc:
    best_clf = nb_clf
    print("Selected Naive Bayes")
elif best_acc == svm_acc:
    best_clf = svm_clf
    print("Selected SVM")
elif best_acc == lr_acc:
    best_clf = lr_clf
    print("Selected Logistic Regression")
else:
    best_clf = knn_clf
    print("Selected kNN")

# Deploy the best performing classifier
best_clf.fit(tfidf_matrix, labels)

# import tkinter as tk
# from tkinter import simpledialog
# ROOT = tk.Tk()

# ROOT.withdraw()
# input_text = simpledialog.askstring(title="Please vent T_T")

input_text = input()
#Vectorise the input

input_vector = tfidf_vectorizer.transform([input_text])
# Make predictions
prediction = best_clf.predict(input_vector)

# Output the result
if prediction == 1:
    print("You may be experiencing symptoms of depression. Please seek professional help.")
else:
    print("You seem to be doing fine. Keep up the good work!")

# input_text1 = nltk.word_tokenize(input_text.lower())
# # input_text = [input_text for input_text in input_text1 if input_text in english_words]
# # input_text1 ' '.join(input_text1)
# def clean_user_input(text):
#     # Remove URLs
#     text = re.sub(r'http\S+', '', text)
#     # Remove non-English characters and symbols
#     text = re.sub(r'[^\x00-\x7F]+', ' ', text)
#     text = re.sub(r'[^\w\s]', ' ', text)
#     # Remove extra whitespace
#     text = re.sub(r'\s+', ' ', text).strip()
#     # Remove non-English words
#     tokens = nltk.word_tokenize(text.lower())
#     tokens = [token for token in tokens if token in english_words]
#     return ' '.join(tokens)

#Clean text
# cleaned_input = clean_user_input(input_text)

# Vectorize the input data
# Use our selected classifier to predict the label
# predicted_label = best_clf.predict(input_vector)
# print(predicted_label)

# Get the feature names corresponding to the non-zero elements in the tf-idf vector
# feature_indices = input_vector.nonzero()[1]
# feature_names = tfidf_vectorizer.get_feature_names_out()

# # Assume that you have already loaded the best performing classifier (best_clf) and the trained vectorizer (vectorizer)

# # Collect input data
# feeling = input("How are you feeling today? ")
# sleeping = input("How has your sleeping been lately? ")
# energy = input("How are your energy levels throughout the day? ")
# activity = input("Do you prefer going out or staying in? ")
# value = input("Do you sometimes feel that you are not valuable enough? ")

# # Clean the input data
# input_text = feeling + " " + sleeping + " " + energy + " " + activity + " " + value
# input_text = re.sub(r'http\S+', '', input_text)
# input_text = BeautifulSoup(input_text, 'html.parser').get_text()
# input_text = input_text.replace('\n', ' ')
# input_text = input_text.replace('\\', '')
# tokens = word_tokenize(input_text.lower())
# tokens = [word for word in tokens if not word in stopwords.words()]
# cleaned_input = ' '.join(tokens)

# # Vectorize the input data
# input_vector = vectorizer.transform([cleaned_input])

# # Make predictions
# prediction = best_clf.predict(input_vector)

# # Output the result
# if prediction[0] == 1:
#     print("You may be experiencing symptoms of depression. Please seek professional help.")
# else:
#     print("You seem to be doing fine. Keep up the good work!")



# # Deploying the model 

# input_list = []
# input_text1 = input("How are you feeling today?")
# input_text2 = input("Have you been stressed and anxious lately")
# input_text3 = input("How would you describe your sleep?")
# input_text4 = input("How would you describe energy throughout the day?")
# input_text5 = input("Do you prefer to stay at home rather than going out and doing new things?")
# input_list = [input_text1, input_text2, input_text3, input_text4, input_text5]

# cleaned_input_list = []
# for text in input_list:
#     text = re.sub(r'http\S+', '', text)
#     text = BeautifulSoup(text, 'html.parser').get_text()
#     text = text.replace('\n', ' ')
#     text = text.replace('\\', '')
#     # Tokenize the text and remove stop words
#     input_tokens = word_tokenize(text.lower())
#     input_tokens = [word for word in tokens if not word in stopwords.words()]
#     # Append the cleaned and tokenized text to the cleaned list
#     cleaned_input_list.append(' '.join(input_tokens))
# cleaned_input_list= ''.join(cleaned_input_list)



# tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))
# tfidf_vectorizer.fit(all_tweets)
# vectorized_list = tfidf_vectorizer.fit_transform(cleaned_input_list)


# #predict
# predicted_label = best_clf.predict(vectorized_list)

# # Print the predicted label
# print(predicted_label)

# import nltk
# from nltk.corpus import words

# # Download the English words corpus
# nltk.download('words')
# english_words = set(words.words())
# negative = []
# # Clean the text
# for text in negative_tweets:
#     words_list = text.lower().split()  # Split the text into words and convert to lowercase
#     negative += [word for word in words_list if word in english_words]  # Keep only English words
    
# negative

# negative_tweets[-1]

